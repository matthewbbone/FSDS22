{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f94b4d7-69b5-442b-a9e6-c6653fcb5aa3",
   "metadata": {},
   "source": [
    "# Fundamentals of Social Data Science \n",
    "\n",
    "Week 1. Day 2. Exercises from Chapter 2 of FSStDS \n",
    "\n",
    "Within your study pod discuss the following questions. Please submit an individual assignment by 12:30pm tomorrow, Wednesday October 12, 2022 on Canvas. \n",
    "\n",
    "These will not be marked and are solely for recordkeeping and review upon request. They will, however, be discussed in the Wednesday tutorial and briefing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "40bc89e8-9c87-48b4-9e72-b77a16c95fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd6bf57-fa7d-43c7-a9a4-148f0144045c",
   "metadata": {},
   "source": [
    "# Alice's Adventures in Text\n",
    "\n",
    "Since Alice's Adventures in Wonderland is in the public domain, we can rightly access it and make use of it as data with few issues. Below I have a code snippet that will download a version of AAiW, load the result as a string and split that string (by space, as is default) into a list of words. The rest is up to you. See questions below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a821d34d-d9f7-44b3-958f-00d305897ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "alice_path = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "\n",
    "req = requests.get(alice_path)\n",
    "\n",
    "text = req.content.decode(\"utf-8\") \n",
    "\n",
    "words = text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1791cd5f-5181-4132-b5cb-3e24ea1e4ab3",
   "metadata": {},
   "source": [
    "# Block 1. Simple descriptive reporting\n",
    "\n",
    "First let's start by turning that list `words` into a Series and then doing some descriptives on this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d4cd0c9f-07e1-451c-bad2-3b2dbc2acb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29594 words in this file.\n",
      "There are 5981 unique words in this file.\n",
      "The top ten words are as follows: was, it, in, said, she, of, a, to, and, the\n"
     ]
    }
   ],
   "source": [
    "# 1a. How many words are in this text file? \n",
    "# 1b. How many unique words are in this text file? \n",
    "# 1c. What are the top ten words. Note, you can get this from a value_counts(), but it will not print those ten words with display(ser.value_counts()). How will you find those words and display them? (Hint - a value_counts() itself returns a Series).\n",
    "\n",
    "alice_ser = pd.Series(words)\n",
    "len_ser = len(alice_ser)\n",
    "alice_ser_ucount = len(alice_ser.unique())\n",
    "top10_words = ', '.join(alice_ser.value_counts().sort_values().tail(10).index)\n",
    "\n",
    "print(f\"There are {len_ser} words in this file.\")\n",
    "print(f\"There are {alice_ser_ucount} unique words in this file.\")\n",
    "print(f\"The top ten words are as follows: {top10_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e765e18-ae67-4c50-9421-25015119cde7",
   "metadata": {},
   "source": [
    "## Block 1a. Masking and filtering \n",
    "\n",
    "So instead of reporting on all the words, let's B more selective. Filter all the words down to whether they have the letter 'b' in them. Remember to check for case sensitivity? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bc2f8298-f10b-47c3-80a3-b34c3bb488d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1550 words with the letter 'b' in them (regardless of case\n",
      "Of the words in the Series, 5.2% have the letter 'b'\n"
     ]
    }
   ],
   "source": [
    "# 1d. How many 'b' words are in the file? \n",
    "# 1e. What percent of words in the file contain the letter 'b'?\n",
    "\n",
    "# Step 1. Get 'b' in each row using map-lambda (or other ways)\n",
    "bmask = alice_ser.map(lambda x: x if 'b' in x else float('NaN')).dropna()\n",
    "\n",
    "# Step 2. Get length of filtered Series\n",
    "bword_count = len(bmask)\n",
    "bword_percent = bword_count / len(alice_ser) \n",
    "\n",
    "print(f\"There are {bword_count} words with the letter 'b' in them (regardless of case\")\n",
    "\n",
    "print(f\"Of the words in the Series, {bword_percent:0.1%} have the letter 'b'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfa1662-3821-4be1-b853-fef6ca010037",
   "metadata": {},
   "source": [
    "# Block 2. Using Map and lambda to clean the series\n",
    "\n",
    "Create a new series `alice_ser_c` which is a cleaned version of the series. To clean it, for each word,\n",
    "- Transform it into lower case \n",
    "- Remove punctuation from the front and the rear of the word. Note, this can be done in many ways. Some of these are rolled into the natural language toolkit, but I would like us to think about how to do this by hand. You can try the code from: https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string however, note that this strips **all** punctuation, so \"I'd\" and \"id\" would be treated as the same word. In the answer I gave an example of 'the hard way'. Give yourself some limited time, try a simple way and then check the answer.\n",
    "- Return to the data and provide some new, comparative, descriptive statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e358eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a5aa283b-c754-4a75-9b66-ebacdd8d81f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once converted to lower case, there are 5650 unique words in the Series\n",
      "Once punctuation is stripped, there are 3226 unique words in the Series\n",
      "In the cleaned Series the top ten most common words are:\n",
      "in, you, said, she, it, of, a, to, and, the\n"
     ]
    }
   ],
   "source": [
    "# 2a. Create a map statement that will \n",
    "#     return a new series with all letters in lower case.\n",
    "#     Report on the unique words now. \n",
    "# 2b. [Challenge] Create a map statement that will return the lower case word \n",
    "#     without punctation at the front and back of the word.\n",
    "#     With both punctuation and lower case, report on unique words\n",
    "# 2c. Compare the top ten words in this new Series with the original\n",
    "#     What words appear in the new top ten? What might explain this?\n",
    "\n",
    "def strip_punct(text):\n",
    "\n",
    "    matches = re.finditer('[a-zA-Z]', text)\n",
    "    \n",
    "    try: \n",
    "        first = next(matches)\n",
    "        *_, last = matches\n",
    "    except StopIteration as e:\n",
    "        # in this case there were no matches\n",
    "        return float('NaN')\n",
    "    except ValueError as e:\n",
    "        # in this case there was only one match\n",
    "        return text[first.start():first.end()]\n",
    "        \n",
    "    return text[first.start():last.end()]\n",
    "\n",
    "    \n",
    "alice_ser_lower = alice_ser.map(lambda x: x.lower())\n",
    "alice_sl_ucount = len(alice_ser_lower.unique())\n",
    "alice_ser_c = alice_ser.map(lambda x: strip_punct(x.lower()))\n",
    "\n",
    "alice_ser_c.dropna(inplace=True) # Get rid of rows where there were no\n",
    "                                 # alphanumeric characters\n",
    "\n",
    "alice_sc_ucount = len(alice_ser_c.unique())\n",
    "c_top10_words = ', '.join(alice_ser_c.value_counts().sort_values().tail(10).index)\n",
    "\n",
    "print(f\"Once converted to lower case, there are {alice_sl_ucount} unique words in the Series\")\n",
    "print(f\"Once punctuation is stripped, there are {alice_sc_ucount} unique words in the Series\") \n",
    "\n",
    "print(f\"In the cleaned Series the top ten most common words are:\\n{c_top10_words}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6e5e35af-495b-4880-8ca3-108a58e5fc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is True that words with 'b' are longer on average. 'b' words are an average 5.7 characters in length. The others are an average 4.1 characters in length.\n"
     ]
    }
   ],
   "source": [
    "# 2d. Now that we have a more cleaned data set, lets compare word lengths\n",
    "#     In this cleaned Series are the words with b on average longer or shorter\n",
    "#     than the words without a 'b'? \n",
    "\n",
    "# Step 1. Get mask for b words - get new series\n",
    "\n",
    "hasb_ser =  alice_ser_c.map(lambda x: x if 'b' in x else float('NaN')).dropna()\n",
    "\n",
    "# Step 1a. Get new series with the length of these words\n",
    "len_hasb_ser = hasb_ser.map(len) # A new series of lengths of these words\n",
    "\n",
    "# Step 1b. Get the average length \n",
    "avg_len_hasb = len_hasb_ser.mean()\n",
    "\n",
    "#Step 2. Get mask for non-b words - get new series\n",
    "notb_ser = alice_ser_c.map(lambda x: float('NaN') if 'b' in x else x).dropna()\n",
    "\n",
    "# Step 2a. \n",
    "len_notb_ser = notb_ser.map(len) # A new series of lengths of these words\n",
    "\n",
    "# Step 2b. \n",
    "avg_len_nonb = len_notb_ser.mean()\n",
    "\n",
    "bword_longer = avg_len_hasb > avg_len_nonb # A Boolean representing if b words are longer on avg.\n",
    "\n",
    "print(f\"It is {bword_longer} that words with 'b' are longer on average.\",\n",
    "      f\"'b' words are an average {avg_len_hasb:0.1f} characters in length.\",\n",
    "      f\"The others are an average {avg_len_nonb:0.1f} characters in length.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a5e53b-84f6-4801-85de-3d243bdaee17",
   "metadata": {},
   "source": [
    "# Block 3. From data back to phenomena \n",
    "\n",
    "We took the data from Project Guttenberg as is and then acted upon this as if it were Alice's Adventures in Wonderland. But is it true? Look at the file in a browser. It has a header, author names, details at the bottom, etc... Further, you might notice that there is at least one kind of punctuation that links two separate words but has no space in between. \n",
    "\n",
    "Below this is partially a question for discussion: How can we clean the data so that the string which is converted to a Series represents the text of the book and not the front and back matter? Later on there are techniques called regular expressions which might help, but I think we can get away with stripping that text without regex. \n",
    "\n",
    "Below, propose an approach that will strip out the top and bottom text from the .txt file. Consider an approach that might be more general than for this specific text file but more general to text files from Project Guttenberg. For example, would it work with the text from Wilde's A Picture of Dorian Gray? See: https://www.gutenberg.org/cache/epub/174/pg174.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6aacec93-2301-4676-b128-f909d8b98dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By removing the license from the text we process, we have removed 3069 words from the Alice file prior to processing.\n",
      "By removing the license from the text we process, we have removed 3048 words from the Dorian Gray file prior to processing.\n"
     ]
    }
   ],
   "source": [
    "# 3a. How many words have we removed by stripping out the license?\n",
    "#     Note, this assumes the same simple split by space\n",
    "\n",
    "def remove_gutenberg_license(text):\n",
    "    '''Removes the licence text from the top and bottom of files \n",
    "       downloaded from Project Gutenberg''' \n",
    "\n",
    "    lines = pd.Series(text.split('\\n'))\n",
    "    line_bookends = lines.map(lambda x: x[0:9] == '*** START' or x[0:7] == '*** END')\n",
    "    start_end = line_bookends[line_bookends].index\n",
    "    \n",
    "    return '\\n'.join(lines[(start_end[0] + 1):start_end[1]])\n",
    "\n",
    "alice_path = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "doriangray_path = \"https://www.gutenberg.org/cache/epub/174/pg174.txt\"\n",
    "\n",
    "req = requests.get(alice_path)\n",
    "text = req.content.decode(\"utf-8\")\n",
    "new_text = remove_gutenberg_license(text)\n",
    "less_words_alice = len(text.split()) - len(new_text.split())\n",
    "\n",
    "req = requests.get(doriangray_path)\n",
    "text = req.content.decode(\"utf-8\")\n",
    "new_text = remove_gutenberg_license(text)\n",
    "less_words_dg = len(text.split()) - len(new_text.split())\n",
    "\n",
    "print(f\"By removing the license from the text we process, we have removed {less_words_alice} words from the Alice file prior to processing.\")\n",
    "print(f\"By removing the license from the text we process, we have removed {less_words_dg} words from the Dorian Gray file prior to processing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fsd22env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "72ab57b0b5f2fd64aef2c7b60b3862b1a450b7cb33941b7430bf767e797b1ae3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
