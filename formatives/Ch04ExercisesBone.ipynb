{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb9d7bac-c8a9-4221-9d42-0b91d8f05a00",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Week 2. Day 1. Exercises from Chapter 4 of FSStDS. \n",
    "## Fundamentals of Social Data Science. MT 2022\n",
    "\n",
    "Within your study pod discuss the following questions. Please submit an individual assignment by 12:30pm Tuesday, October 18, 2022 on Canvas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803932b-ef93-4423-970d-fd1d20f5c82a",
   "metadata": {},
   "source": [
    "# Exercise 1. Creating a DataFrame from multiple JSON files\n",
    "\n",
    "There are nine pages of search results for Oxford from OMDB (as of last year; `omdb_Oxford_search_page_\\*.json`). \n",
    "\n",
    "**Exercise 1a.** Create a single DataFrame from these 9 files.\n",
    "\n",
    "**Exercise 1b.** Report on the oldest and most recent entry. \n",
    "\n",
    "- **Hint**. To read all files from a Path object with a wildcard use the 'glob' method, such as: `for path in data_dir.blog(\"omdb_Oxford*.json\"): path.do_something()`\n",
    "\n",
    "- **Challenge** - Note that shows that span years are written with the two years separated by `--`. So ensure that you split this and then consider these years when reporting the oldest and newest entries. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "42348a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pathlib as p\n",
    "import re\n",
    "import xmltodict\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5964f5b7-64a6-4bf6-b331-b38c7e4e90f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>imdbID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Poster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oxford Circus</td>\n",
       "      <td>1897</td>\n",
       "      <td>tt2323621</td>\n",
       "      <td>movie</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oxford and Cambridge Boat Race</td>\n",
       "      <td>1900</td>\n",
       "      <td>tt2371366</td>\n",
       "      <td>movie</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jesus Is My Rock: A Celebration of Gospel Musi...</td>\n",
       "      <td>2012</td>\n",
       "      <td>tt2404172</td>\n",
       "      <td>movie</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oxford and Cambridge Boat Race 1901</td>\n",
       "      <td>1901</td>\n",
       "      <td>tt2491060</td>\n",
       "      <td>movie</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>International Field Sports: Oxford-Cambridge v...</td>\n",
       "      <td>1901</td>\n",
       "      <td>tt2549482</td>\n",
       "      <td>movie</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  Year     imdbID   Type  \\\n",
       "0                                      Oxford Circus  1897  tt2323621  movie   \n",
       "1                     Oxford and Cambridge Boat Race  1900  tt2371366  movie   \n",
       "2  Jesus Is My Rock: A Celebration of Gospel Musi...  2012  tt2404172  movie   \n",
       "3                Oxford and Cambridge Boat Race 1901  1901  tt2491060  movie   \n",
       "4  International Field Sports: Oxford-Cambridge v...  1901  tt2549482  movie   \n",
       "\n",
       "  Poster  \n",
       "0    N/A  \n",
       "1    N/A  \n",
       "2    N/A  \n",
       "3    N/A  \n",
       "4    N/A  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 1a answer below here \n",
    "json_list = []\n",
    "for path in p.Path(\"../data/ch04/\").glob('omdb_Oxford*.json'):\n",
    "    file = open(path)\n",
    "    json_list = json_list + json.load(file)['Search']\n",
    "    \n",
    "omdb_df = pd.DataFrame(json_list)\n",
    "omdb_df.head(5)\n",
    "# Exercise 1a answer above here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6da13185-3935-4c3b-b45b-12605ff78a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Oxford and Cambridge University Boat Race is the oldest entry, and came out in 1895\n",
      "Oxford Street 24/7 is the newest entry, and came out in 2018–2019\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1b answer below here \n",
    "oldest_row = omdb_df.sort_values(by=['Year']).head(1)\n",
    "newest_row = omdb_df.sort_values(by=['Year']).tail(1)\n",
    "\n",
    "print(f\"{oldest_row['Title'].iloc[0]} is the oldest entry, and came out in {oldest_row['Year'].iloc[0]}\")\n",
    "print(f\"{newest_row['Title'].iloc[0]} is the newest entry, and came out in {newest_row['Year'].iloc[0]}\")\n",
    "# Exercise 1b answer above here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ed32db-97d9-4a31-9622-b1a654dd79d6",
   "metadata": {},
   "source": [
    "# Exercise 2. Navigate Reddit JSON \n",
    "\n",
    "Go to a page on reddit and then replace www.reddit with api.reddit. This will then give the page as JSON. Do this for a specific subreddit of interest (such as cats, cryptocurrency, mediasynthesis, ukpolitics, etc...). \n",
    "\n",
    "This json will likely only have 25-26 entries. Normalise by data so that each story has a single line. This will have many, many columns. One of these columns will be the title of the headline and one will be the URL. \n",
    "\n",
    "- **Exercise 2a**. Find these two columns and then create a smaller DataFrame that just has these columns as well as the one for upvote score (`ups`).  \n",
    "\n",
    "- **Exercise 2b**. What are the most common words across all titles? Does it matter if you use lower case and remove punctuation as we did last week? \n",
    "\n",
    "- **Exercise 2c**. What domain names are the most common?\n",
    "\n",
    "> **Hint**: If you aren't having luck with saving your own JSON, you can use the old `environment.json` that is appended with the data. \n",
    "\n",
    "> **Hint**: Parsing domain names can be a nuisance. Here is a a small snippet that can help: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "750c3cb3-1f0c-41f9-b9c9-32ee0d180e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParseResult(scheme='http', netloc='www.nytimes.com', path='/somestory.html', params='', query='', fragment='')\n"
     ]
    }
   ],
   "source": [
    "# See: https://docs.python.org/3/library/urllib.parse.html\n",
    "# For example:\n",
    "from urllib.parse import urlparse\n",
    "result = urlparse(\"http://www.nytimes.com/somestory.html\")\n",
    "print(result) # Which item is the domain name? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "cad7d8ff-ebcb-4c8f-a4bc-33ea9a7bc810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Url</th>\n",
       "      <th>Upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Halloween Megathread 2022</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sexy Halloween VIII: Sexurrection</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the stupidest thing a large amount of ...</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>17879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the unwritten rules of Reddit?</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>6749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What famous person do you dislike, why?</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Who are the scariest types of people?</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>15575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is cool in the eyes of most high schooler...</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>5439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What’s something you wouldn’t even try once?</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y6...</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what's something people don't think about when...</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y6...</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Non British folk, what do you assume that us B...</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Non-Americans, what do you think every America...</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>42527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What would you do in your 20s to avoid regret ...</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What deceased musician would still be making a...</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>1375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What life-changing item can you buy for less t...</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>4191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What is legal and should become illegal?</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>10086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(serious) What are You struggling with right now?</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[Serious] What’s a food that you hate the text...</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What’s a word you can never remember how to sp...</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What's the game you've put 1000 hours in?</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y6...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What is the biggest waste of money?</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What's the one thing that you wish was real?</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y6...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Non-Canadians, what do you think every Canadia...</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>If you were given 5 minutes in a supermarket t...</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y6...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What is the fastest way of losing weight?</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y6...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What did you want to be when you grew up versu...</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y6...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>“I’ll be there in 5 minutes” is a universal li...</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y6...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>If men stare at “ass and titties”, what do wom...</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/y5...</td>\n",
       "      <td>7912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0                           Halloween Megathread 2022   \n",
       "1                   Sexy Halloween VIII: Sexurrection   \n",
       "2   What is the stupidest thing a large amount of ...   \n",
       "3             What are the unwritten rules of Reddit?   \n",
       "4             What famous person do you dislike, why?   \n",
       "5               Who are the scariest types of people?   \n",
       "6   What is cool in the eyes of most high schooler...   \n",
       "7        What’s something you wouldn’t even try once?   \n",
       "8   what's something people don't think about when...   \n",
       "9   Non British folk, what do you assume that us B...   \n",
       "10  Non-Americans, what do you think every America...   \n",
       "11  What would you do in your 20s to avoid regret ...   \n",
       "12  What deceased musician would still be making a...   \n",
       "13  What life-changing item can you buy for less t...   \n",
       "14           What is legal and should become illegal?   \n",
       "15  (serious) What are You struggling with right now?   \n",
       "16  [Serious] What’s a food that you hate the text...   \n",
       "17  What’s a word you can never remember how to sp...   \n",
       "18          What's the game you've put 1000 hours in?   \n",
       "19                What is the biggest waste of money?   \n",
       "20       What's the one thing that you wish was real?   \n",
       "21  Non-Canadians, what do you think every Canadia...   \n",
       "22  If you were given 5 minutes in a supermarket t...   \n",
       "23          What is the fastest way of losing weight?   \n",
       "24  What did you want to be when you grew up versu...   \n",
       "25  “I’ll be there in 5 minutes” is a universal li...   \n",
       "26  If men stare at “ass and titties”, what do wom...   \n",
       "\n",
       "                                                  Url  Upvotes  \n",
       "0   https://www.reddit.com/r/AskReddit/comments/y5...       38  \n",
       "1   https://www.reddit.com/r/AskReddit/comments/y5...        0  \n",
       "2   https://www.reddit.com/r/AskReddit/comments/y5...    17879  \n",
       "3   https://www.reddit.com/r/AskReddit/comments/y5...     6749  \n",
       "4   https://www.reddit.com/r/AskReddit/comments/y5...     2378  \n",
       "5   https://www.reddit.com/r/AskReddit/comments/y5...    15575  \n",
       "6   https://www.reddit.com/r/AskReddit/comments/y5...     5439  \n",
       "7   https://www.reddit.com/r/AskReddit/comments/y6...      640  \n",
       "8   https://www.reddit.com/r/AskReddit/comments/y6...      141  \n",
       "9   https://www.reddit.com/r/AskReddit/comments/y5...      308  \n",
       "10  https://www.reddit.com/r/AskReddit/comments/y5...    42527  \n",
       "11  https://www.reddit.com/r/AskReddit/comments/y5...      975  \n",
       "12  https://www.reddit.com/r/AskReddit/comments/y5...     1375  \n",
       "13  https://www.reddit.com/r/AskReddit/comments/y5...     4191  \n",
       "14  https://www.reddit.com/r/AskReddit/comments/y5...    10086  \n",
       "15  https://www.reddit.com/r/AskReddit/comments/y5...      643  \n",
       "16  https://www.reddit.com/r/AskReddit/comments/y5...      959  \n",
       "17  https://www.reddit.com/r/AskReddit/comments/y5...      425  \n",
       "18  https://www.reddit.com/r/AskReddit/comments/y6...       27  \n",
       "19  https://www.reddit.com/r/AskReddit/comments/y5...       65  \n",
       "20  https://www.reddit.com/r/AskReddit/comments/y6...       11  \n",
       "21  https://www.reddit.com/r/AskReddit/comments/y5...       71  \n",
       "22  https://www.reddit.com/r/AskReddit/comments/y6...       10  \n",
       "23  https://www.reddit.com/r/AskReddit/comments/y6...       47  \n",
       "24  https://www.reddit.com/r/AskReddit/comments/y6...       10  \n",
       "25  https://www.reddit.com/r/AskReddit/comments/y6...       10  \n",
       "26  https://www.reddit.com/r/AskReddit/comments/y5...     7912  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 2a Answer below here \n",
    "file = open('../data/ch04/reddit.json')\n",
    "entries = json.load(file)['data']['children']\n",
    "entry_list = []\n",
    "for entry in entries:\n",
    "    temp_dict = {}\n",
    "    temp_dict['Title'] = entry['data']['title']\n",
    "    temp_dict['Url'] = entry['data']['url']\n",
    "    temp_dict['Upvotes'] = entry['data']['ups']\n",
    "    entry_list.append(temp_dict)\n",
    "\n",
    "reddit_df = pd.DataFrame(entry_list)\n",
    "reddit_df\n",
    "# Exercise 2a Answer above here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f567c630-eb57-4a00-b683-4095b33903b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the titles are uncleaned, 'you' is the most common word with 17 instances\n",
      "When the titles are cleaned, 'what' is the most common word with 20 instances\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2b Answer below here \n",
    "\n",
    "single_string = ' '.join(reddit_df['Title'].tolist())\n",
    "unclean_common = pd.Series(single_string.split(' ')).value_counts().head(1)\n",
    "clean_common = pd.Series(re.sub('[^a-zA-Z\\d\\s]', '', single_string).lower().split(' ')).value_counts().head(1)\n",
    "\n",
    "print(f\"When the titles are uncleaned, '{unclean_common.keys()[0]}' is the most common word with {unclean_common[0]} instances\")\n",
    "print(f\"When the titles are cleaned, '{clean_common.keys()[0]}' is the most common word with {clean_common[0]} instances\")\n",
    "# Exercise 2b Answer above here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "71d9d55f-93d0-417d-923e-f9ea61353061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.reddit.com is the most common domain with 27 instances (it's also the only domain?)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2c Answer below here \n",
    "\n",
    "common_domain = pd.Series([url.split('/')[2] for url in reddit_df['Url'].tolist()]).value_counts()\n",
    "print(f\"{common_domain.keys()[0]} is the most common domain with {common_domain[0]} instances (it's also the only domain?)\")\n",
    "# Exercise 2c Answer above here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b17f7-d9b2-40ff-b569-7a5184786f44",
   "metadata": {},
   "source": [
    "# Exercise 3. The love-hate relationship with DIKW\n",
    "\n",
    "As mentioned in the chapter, the Wikipedia entry for data had DIKW in the article, then it was removed, then it reappared! I think it is still there now. I did not do the editing of this. \n",
    "\n",
    "With the data export of the Wikipedia page on data (`Wikipedia - data - Special export - 2022-10-17_10_24_15.xml`): \n",
    "\n",
    "- **Exercise 3a**. Create a DataFrame where each revision of the Wikipedia article in the export is given its own row. \n",
    "- **Exercise 3b**. Search for the first time DIKW was mentioned and the last time it was mentioned. Try to find the gap? When did it appear? \n",
    "\n",
    "> **Hint**: Using `xmltodict` might be helpful for wrangling the XML data, but it might also make life complicated. Explore the data both through a text editor (or browser) and through code to get a sense of it. \n",
    "\n",
    "> **Hint**: It is admittedly a little easier to do this if you make use of time in your DataFrame. We do not cover that much until Chapter 10, but feel free to look ahead. You can still sort by revisionID and then just browse the data yourself. This will end up being one of those tasks that's not easy but gets easier with more skills of abstraction.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "544c55c4-5f10-4992-b11d-795d5eda84f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>contributor</th>\n",
       "      <th>minor</th>\n",
       "      <th>comment</th>\n",
       "      <th>model</th>\n",
       "      <th>format</th>\n",
       "      <th>text</th>\n",
       "      <th>sha1</th>\n",
       "      <th>parentid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>246479</td>\n",
       "      <td>2001-03-17T06:29:52Z</td>\n",
       "      <td>{'username': '208.245.214.xxx', 'id': '0'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*</td>\n",
       "      <td>wikitext</td>\n",
       "      <td>text/x-wiki</td>\n",
       "      <td>{'@bytes': '542', '@xml:space': 'preserve', '#...</td>\n",
       "      <td>4bi0lqmoh3d6z1tv2dg3zfjaqt20fj3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>246480</td>\n",
       "      <td>2001-03-26T22:35:32Z</td>\n",
       "      <td>{'username': '192.75.241.xxx', 'id': '0'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*</td>\n",
       "      <td>wikitext</td>\n",
       "      <td>text/x-wiki</td>\n",
       "      <td>{'@bytes': '559', '@xml:space': 'preserve', '#...</td>\n",
       "      <td>lv1z2dzhms5ys77nfrgq58mem9ptdik</td>\n",
       "      <td>246479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18191</td>\n",
       "      <td>2002-02-25T14:51:43Z</td>\n",
       "      <td>{'username': 'Conversion script', 'id': '12264...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Automated conversion</td>\n",
       "      <td>wikitext</td>\n",
       "      <td>text/x-wiki</td>\n",
       "      <td>{'@bytes': '556', '@xml:space': 'preserve', '#...</td>\n",
       "      <td>rtvjc8fo6z391pjlte9kw8e7wi02enn</td>\n",
       "      <td>246480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18192</td>\n",
       "      <td>2002-02-25T14:52:12Z</td>\n",
       "      <td>{'ip': '213.253.40.156'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>term in bold</td>\n",
       "      <td>wikitext</td>\n",
       "      <td>text/x-wiki</td>\n",
       "      <td>{'@bytes': '565', '@xml:space': 'preserve', '#...</td>\n",
       "      <td>1x390dickut12wma4fuy2eezu138o37</td>\n",
       "      <td>18191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18817</td>\n",
       "      <td>2002-02-25T15:43:11Z</td>\n",
       "      <td>{'ip': '213.253.40.156'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linking mass noun</td>\n",
       "      <td>wikitext</td>\n",
       "      <td>text/x-wiki</td>\n",
       "      <td>{'@bytes': '569', '@xml:space': 'preserve', '#...</td>\n",
       "      <td>gprvn7s0a4k8jrjcou6lq139xxv1hde</td>\n",
       "      <td>18192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             timestamp  \\\n",
       "0  246479  2001-03-17T06:29:52Z   \n",
       "1  246480  2001-03-26T22:35:32Z   \n",
       "2   18191  2002-02-25T14:51:43Z   \n",
       "3   18192  2002-02-25T14:52:12Z   \n",
       "4   18817  2002-02-25T15:43:11Z   \n",
       "\n",
       "                                         contributor  minor  \\\n",
       "0         {'username': '208.245.214.xxx', 'id': '0'}    NaN   \n",
       "1          {'username': '192.75.241.xxx', 'id': '0'}    NaN   \n",
       "2  {'username': 'Conversion script', 'id': '12264...    NaN   \n",
       "3                           {'ip': '213.253.40.156'}    NaN   \n",
       "4                           {'ip': '213.253.40.156'}    NaN   \n",
       "\n",
       "                comment     model       format  \\\n",
       "0                     *  wikitext  text/x-wiki   \n",
       "1                     *  wikitext  text/x-wiki   \n",
       "2  Automated conversion  wikitext  text/x-wiki   \n",
       "3          term in bold  wikitext  text/x-wiki   \n",
       "4     linking mass noun  wikitext  text/x-wiki   \n",
       "\n",
       "                                                text  \\\n",
       "0  {'@bytes': '542', '@xml:space': 'preserve', '#...   \n",
       "1  {'@bytes': '559', '@xml:space': 'preserve', '#...   \n",
       "2  {'@bytes': '556', '@xml:space': 'preserve', '#...   \n",
       "3  {'@bytes': '565', '@xml:space': 'preserve', '#...   \n",
       "4  {'@bytes': '569', '@xml:space': 'preserve', '#...   \n",
       "\n",
       "                              sha1 parentid  \n",
       "0  4bi0lqmoh3d6z1tv2dg3zfjaqt20fj3      NaN  \n",
       "1  lv1z2dzhms5ys77nfrgq58mem9ptdik   246479  \n",
       "2  rtvjc8fo6z391pjlte9kw8e7wi02enn   246480  \n",
       "3  1x390dickut12wma4fuy2eezu138o37    18191  \n",
       "4  gprvn7s0a4k8jrjcou6lq139xxv1hde    18192  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3a Answer below here\n",
    "\n",
    "file = open('../data/ch04/wikipedia.xml')\n",
    "xml_dict = xmltodict.parse(file.read())\n",
    "revisions = xml_dict['mediawiki']['page']['revision']\n",
    "\n",
    "revise_df = pd.DataFrame(revisions)\n",
    "revise_df.head(5)\n",
    "\n",
    "# EXercise 3a Answer above here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fb4d4201-a466-4fe1-9e35-0a29c43cd795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-08-25 is the first day DIKW appeared in a revision, 2005-11-23 is the last day. There was a gap of 90 days\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3b Answer below here\n",
    "\n",
    "times = []\n",
    "for revise in revisions:\n",
    "    try: \n",
    "        has_dikw = re.search('DIKW', revise['text']['#text'])\n",
    "        if has_dikw: \n",
    "            times.append(revise['timestamp'])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "format = \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "first_date = dt.strptime(times[0], format)\n",
    "last_date = dt.strptime(times[-1], format)\n",
    "\n",
    "print(f\"{first_date.date()} is the first day DIKW appeared in a revision, {last_date.date()} is the last day. There was a gap of {(last_date - first_date).days} days\")\n",
    "\n",
    "# EXercise 3b Answer above here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fsd22env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "72ab57b0b5f2fd64aef2c7b60b3862b1a450b7cb33941b7430bf767e797b1ae3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
